title: SVM总结

tags: [机器学习,svm,总结]

date: 2015-04-10 22:43:15



---



话说当年知识掌握得还比较轻飘，在某厂面试时被要求徒手推导SVM也是一种难得的体验。



转眼几年过去了，这里懒得再推SVM，就只放个综述作为备忘录。



SVM也是一种基本分类器，与之前提到的感知机(Perceptron)颇为相似，不过需要注意，感知机找到的分类超平面可以有无限多个，只要是能对训练集数据进行正确分类即可，而SVM有着更为苛刻的要求，所求的分类超平面需要离最近的样本数据点距离最大（这些离最终分类平面最近的点被称为支持向量），这就意味着最终找到的分类超平面不仅能够划分训练数据，还需面对未知的数据具有较为良好的分类能力。



<!--more-->



![示意图(来源网络)](\img\2015-04-10-svm-abs-01.jpg)



最后需要解的最优问题：



(1)线性情况:



$$min_\alpha \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} (y_i y_j) (x_i \cdot x_j) - \sum_{i=1}^{N} \alpha_i \\
s.t. \sum_{i=1}^{N} \alpha_{i}y_{i}=0  \\
0<=\alpha_i<=C$$


(2)非线性情况，使用核函数：

$$min_\alpha \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} (y_i y_j) K(x_i \cdot x_j) - \sum_{i=1}^{N} \alpha_i  \\
s.t. \sum_{i=1}^{N} \alpha_{i}y_{i}=0  \\
0<=\alpha_i<=c$$






我的笔记整理如下：

![我的笔记](\img\2015-04-10-svm-abs-02.jpg)


